# -*- coding: utf-8 -*-
"""spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nV3z2DjoM4ZmOHdxQH7G7VvAkkJCcNm6
"""



# # Run the following code block only if you are working in google colab
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget https://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar -zxf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install -q findspark
# import os
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"

# from google.colab import drive
# drive.mount('/content/drive')


# import findspark
# findspark.init()

import pyspark
from pyspark.sql import SparkSession
import numpy as np
import numpy.linalg as la
import matplotlib.pyplot as plt
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
from pyspark.ml.classification import RandomForestClassifier, LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator as MCE
import random

### Problem 1
def word_count(filename='huck_finn.txt'):
  """
  A function that counts the number of occurrences unique occurrences of each
  word. Sorts the words by count in descending order.
  Parameters:
      filename (str): filename or path to a text file
  Returns:
      word_counts (list): list of (word, count) pairs for the 20 most used words
  """ 
  spark = SparkSession\
            .builder\
            .appName("app_name")\
            .getOrCreate()
  # Steps: 
  # 1. read text file
  # 2. Split all lines, flatten
  # 3. Turn each one into dictionary key value pair
  # 4. Sum values by keys
  # 5. Sort
  # 6. Get top 20
  ans = spark.sparkContext.textFile(filename)\
              .flatMap(lambda x: x.split(' '))\
              .map(lambda x: (x, 1))\
              .reduceByKey(lambda x, y: x + y)\
              .sortBy(lambda x: -x[1])\
              .take(20)
  spark.stop()

  return ans

word_count("/content/drive/My Drive/huck_finn.txt")

### Problem 2
def monte_carlo(n=10**5, parts=6):
    """
    Runs a Monte Carlo simulation to estimate the value of pi.
    Parameters:
        n (int): number of sample points per partition
        parts (int): number of partitions
    Returns:
        pi_est (float): estimated value of pi
    """
    spark = SparkSession\
            .builder\
            .appName("app_name")\
            .getOrCreate()
    samples = spark.sparkContext.parallelize(
        np.random.uniform(-1, 1, (int(n * parts), 2)),
        parts)
    norms = samples.map(lambda x: int(la.norm(x) < 1))
    pi_approx = norms.reduce(lambda x, y: x + y) /  (n * parts) * 4

    spark.stop()

    return pi_approx

monte_carlo()

### Problem 3
def titanic_df(filename='titanic.csv'):
    """
    Calculates some statistics from the titanic data.
    
    Returns: the number of women on-board, the number of men on-board,
             the survival rate of women, 
             and the survival rate of men in that order.
    """
    spark = SparkSession\
            .builder\
            .appName("app_name")\
            .getOrCreate()

    schema = ('survived INT, pclass INT, name STRING, sex STRING, '
              'age FLOAT, sibsp INT, parch INT, fare FLOAT')
    titanic = spark.read.csv(filename, schema=schema)

    titanic.createOrReplaceTempView("titanic")
    count_women = spark.sql("""SELECT sex, COUNT(*) as count, MEAN(survived) as survival_rate
                               FROM titanic 
                               GROUP BY sex""")
    arr = np.array(count_women.select("count", "survival_rate").collect())
    spark.stop()

    return arr.T.flatten()

    
titanic_df("/content/drive/My Drive/titanic.csv")

### Problem 4
def crime_and_income(crimefile='london_crime_by_lsoa.csv',
                     incomefile='london_income_by_borough.csv', major_cat='Robbery'):
  """
  Explores crime by borough and income for the specified min_cat
  Parameters:
      crimefile (str): path to csv file containing crime dataset
      incomefile (str): path to csv file containing income dataset
      major_cat (str): crime minor category to analyze
  returns:
      numpy array: borough names sorted by percent months with crime, descending
  """
  spark = SparkSession\
            .builder\
            .appName("app_name")\
            .getOrCreate()
  crime_table = spark.read.csv(crimefile, header=True)
  crime_table.createOrReplaceTempView("crime_table")

  schema = ('borough STRING, mean FLOAT, median FLOAT')
  income_table = spark.read.csv(incomefile, header=True, schema=schema)
  income_table.createOrReplaceTempView("income_table")
  
  crime_selected = spark.sql("""SELECT borough, SUM(value) as total_crime
                                FROM crime_table
                                WHERE major_category = '{0}'
                                GROUP BY borough""".format(major_cat))
  crime_selected.createOrReplaceTempView("crime_selected")

  df = spark.sql("""SELECT c.borough, c.total_crime, i.median
                    FROM crime_selected as c
                    LEFT JOIN income_table as i
                    ON i.borough = c.borough
                    ORDER BY c.total_crime DESC
                    """)
  arr = np.array(df.collect())
  spark.stop()
  return arr

major_cat = "Robbery"
crimes = crime_and_income("/content/drive/My Drive/london_crime_by_lsoa.csv",
                          "/content/drive/My Drive/london_income_by_borough.csv",
                          major_cat)

crime_val = [float(i) for i in crimes[:,1]]
median = [float(i) for i in crimes[:,2]]
plt.scatter(median, crime_val)
plt.xlabel("Median Income")
plt.ylabel("Total {0}".format(major_cat))
plt.title("Total {0} vs. Median Income by Borough".format(major_cat))
plt.show()

### Problem 5
def titanic_classifier(filename='titanic.csv'):
  """
  Implements a classifier model to predict who survived the Titanic.
  Parameters:
      filename (str): path to the dataset
  Returns:
      metrics (list): a list of metrics gauging the performance of the model
          ('accuracy', 'weightedPrecision', 'weightedRecall')
  """
  spark = SparkSession\
          .builder\
          .appName("app_name")\
          .getOrCreate()

  # Load data
  schema = ('survived INT, pclass INT, name STRING, sex STRING, '
            'age FLOAT, sibsp INT, parch INT, fare FLOAT')
  titanic = spark.read.csv(filename, schema=schema)

  # One hot encode pclass column
  onehot = OneHotEncoder(inputCols=['pclass'],
                         outputCols=['pclass_onehot'])
  # Change sex to binary
  sex_binary = StringIndexer(inputCol='sex', outputCol='sex_binary')
  
  # Create features 
  features = ['sex_binary', 'pclass_onehot', 'age', 'sibsp', 'parch', 'fare']
  features_col = VectorAssembler(inputCols=features, outputCol='features')
  pipeline = Pipeline(stages=[sex_binary, onehot, features_col])
  titanic = pipeline.fit(titanic).transform(titanic)

  titanic = titanic.drop('pclass', 'name', 'sex')
  train, test = titanic.randomSplit([0.75, 0.25], seed=11)
  rf = RandomForestClassifier(labelCol='survived', featuresCol='features')
  paramGrid = ParamGridBuilder()\
    .addGrid(rf.numTrees, [50, 100, 200]).build()
  tvs = TrainValidationSplit(estimator=rf,
                              estimatorParamMaps=paramGrid,
                              evaluator=MCE(labelCol='survived'),
                              trainRatio=0.75,
                              seed=11)

  clf = tvs.fit(train)
  results = clf.bestModel.evaluate(test)
  acc = results.accuracy
  res = results.weightedRecall
  pre = results.weightedPrecision
  spark.stop()
  return (acc, res, pre)
titanic_classifier("/content/drive/My Drive/titanic.csv")

